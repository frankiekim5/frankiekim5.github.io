{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "base_path = \"./\"\n",
    "file_path = base_path + \"heart.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "X_data = data.iloc[:, :-1]\n",
    "# print(X_data.shape)\n",
    "Y_data = data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    base_path = \"./\"\n",
    "    file_path = base_path + \"heart.csv\"\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    X_data = data.iloc[:, :-1]\n",
    "    # print(X_data.shape)\n",
    "    Y_data = data.iloc[:, -1]\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled_data = scaler.fit_transform(X_data)\n",
    "    X_data = X_scaled_data\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.2, shuffle=True)\n",
    "\n",
    "    return X_train, X_test, Y_train, Y_test, X_data, Y_data\n",
    "\n",
    "# get_data()\n",
    "\n",
    "def get_dummies(scaler_type):\n",
    "    base_path = \"./\"\n",
    "    file_path = base_path + \"heart.csv\"\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    X_data = data.iloc[:, :-1]\n",
    "    # print(X_data.shape)\n",
    "    Y_data = data.iloc[:, -1]\n",
    "    \n",
    "    feature_names = X_data.columns.tolist()\n",
    "    \n",
    "    if (scaler_type == \"Standard\"):\n",
    "\n",
    "        # Convert categorical variable into dummy/indicator variables.\n",
    "        X_data = pd.get_dummies(X_data, columns=['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'])\n",
    "\n",
    "        # standardscaler removes the mean\n",
    "        # and scales the data to unit variance.\n",
    "        # unit varience = determines the standard deviation of the signal\n",
    "        # and devide all entries by that value.\n",
    "        # it means -> its distribution will have a mean value 0\n",
    "        # and standard deviation of 1.\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        columns_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "        X_data[columns_to_scale] = scaler.fit_transform(X_data[columns_to_scale])\n",
    "\n",
    "    elif (scaler_type == \"MinMax\"):\n",
    "        columns_to_scale = [0, 3, 4, 7, 9]\n",
    "\n",
    "        for c in columns_to_scale:\n",
    "\n",
    "            X_data.iloc[:,c] -= X_data.iloc[:,c].min()\n",
    "            diff = X_data.iloc[:,c].max() - X_data.iloc[:,c].min()\n",
    "            X_data.iloc[:,c] /= diff\n",
    "\n",
    "            column = X_data.iloc[:,c]\n",
    "\n",
    "            for i in range(len(column)):\n",
    "                val = column[i]\n",
    "\n",
    "                if (val >= 0.0 and val < 0.2):\n",
    "                    X_data.iloc[i,c] = 0\n",
    "                elif (val >= 0.2 and val < 0.4):\n",
    "                    X_data.iloc[i,c] = 1\n",
    "                elif (val >= 0.4 and val < 0.6):\n",
    "                    X_data.iloc[i,c] = 2\n",
    "                elif (val >= 0.6 and val < 0.8):\n",
    "                    X_data.iloc[i,c] = 3\n",
    "                else:\n",
    "                    X_data.iloc[i,c] = 4\n",
    "        X_data.to_csv(path_or_buf=\"debug.csv\", decimal=',', index=False)\n",
    "        # X_data = pd.get_dummies(X_data, columns=feature_names)\n",
    "            \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.2, shuffle=True)\n",
    "\n",
    "    return X_train, X_test, Y_train, Y_test, X_data, Y_data\n",
    "\n",
    "def get_data_post_feature_selection():\n",
    "    selected_features = [12, 11, 10, 9, 8, 7, 6, 5, 4, 3]\n",
    "    \n",
    "    selected_features.sort()\n",
    "    \n",
    "    X_data = data.iloc[:, selected_features]\n",
    "    Y_data = data.iloc[:, -1]\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled_data = scaler.fit_transform(X_data)\n",
    "    X_data = X_scaled_data\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.2, shuffle=True)\n",
    "\n",
    "    return X_train, X_test, Y_train, Y_test, X_data, Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from IPython.display import Image  \n",
    "from sklearn import tree\n",
    "import pydotplus\n",
    "\n",
    "def decision_tree():\n",
    "    X_train, X_test, Y_train, Y_test, X_data, Y_data = get_dummies(\"MinMax\")\n",
    "    # feature_names = X_data.columns.tolist()\n",
    "    # print(feature_name)\n",
    "\n",
    "    # scores = {}\n",
    "    scores = []\n",
    "    features = len(X_data.columns)\n",
    "\n",
    "    for i in range(1, features+1):\n",
    "\n",
    "        decision_tree = DecisionTreeClassifier(criterion=\"gini\", max_features=i, random_state=0)\n",
    "        # The number of features to consider when looking for the best split\n",
    "\n",
    "        decision_tree.fit(X_train, Y_train)\n",
    "        Y_pred = decision_tree.predict(X_test)\n",
    "        score = decision_tree.score(X_test, Y_test)\n",
    "        score = round(score, 2)\n",
    "        scores.append(score)\n",
    "        # print(decision_tree.feature_importances_)\n",
    "        feat_importances = pd.Series(decision_tree.feature_importances_, index=X_data.columns)\n",
    "        feat_importances.nlargest(10).plot(kind='barh')\n",
    "        plt.show()\n",
    "\n",
    "    plt.plot([i for i in range(1, features+1)], scores)\n",
    "\n",
    "    for i in range(1, features+1):\n",
    "        plt.text(i, scores[i-1], (i, scores[i-1]))\n",
    "\n",
    "    plt.xticks([i for i in range(1, features+1)])\n",
    "    plt.xlabel('Max features')\n",
    "    plt.ylabel('Scores')\n",
    "    plt.title('Decision Tree Classifier scores for different number of maximum features')\n",
    "    plt.show()\n",
    "\n",
    "    decision_tree_best_features = DecisionTreeClassifier(criterion=\"gini\", max_features=10, random_state=0)\n",
    "    decision_tree_best_features.fit(X_train, Y_train)\n",
    "    score = decision_tree_best_features.score(X_test, Y_test)\n",
    "    print(score)\n",
    "\n",
    "    dot_data = tree.export_graphviz(decision_tree_best_features, out_file=None, \n",
    "                                    # feature_names=names, \n",
    "                                    rounded=True,\n",
    "                                    filled=True)\n",
    "\n",
    "    # Draw graph\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "\n",
    "    # Show graph\n",
    "    Image(graph.create_png())\n",
    "\n",
    "decision_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_selection():\n",
    "    \n",
    "    bestfeatures = SelectKBest(score_func=chi2, k=10)\n",
    "    # SelectKBest = Select features according to the k highest scores.\n",
    "    # chi2 = Compute chi-squared stats between each non-negative feature and class.\n",
    "             \n",
    "            # = the chi-squared test is used to determine \n",
    "            # whether there is a significant difference \n",
    "            # between the expected frequencies and the observed frequencies \n",
    "            # in one or more categories.\n",
    "    fit =  bestfeatures.fit(X_data, Y_data)\n",
    "    scores = pd.DataFrame(fit.scores_)\n",
    "    columns = pd.DataFrame(X_data.columns)\n",
    "    #concat two dataframes for better visualization \n",
    "    featureScores = pd.concat([columns, scores],axis=1)\n",
    "    featureScores.columns = ['Feature','Score']  #naming the dataframe columns\n",
    "    print(featureScores.nlargest(10,'Score'))  #print 10 best features\n",
    "    \n",
    "    # thalach = maximum heart rate achieved\n",
    "    \n",
    "univariate_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance():\n",
    "    \n",
    "    extra_tree = ExtraTreesClassifier()\n",
    "    # This class implements a meta estimator that \n",
    "    # fits a number of randomized decision trees (a.k.a. extra-trees)\n",
    "    # on various sub-samples of the dataset \n",
    "    # and uses averaging to improve the predictive accuracy \n",
    "    # and control over-fitting.\n",
    "    \n",
    "    extra_tree.fit(X_data,Y_data)\n",
    "    print(extra_tree.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "    #plot graph of feature importances for better visualization\n",
    "    feat_importances = pd.Series(extra_tree.feature_importances_, index=X_data.columns)\n",
    "    feat_importances.nlargest(10).plot(kind='barh')\n",
    "    plt.show()\n",
    "    \n",
    "feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heat_map():\n",
    "    \n",
    "    corrmat = data.corr()\n",
    "    # data.corr() finds the pairwise correlation of all columns \n",
    "    \n",
    "    top_corr_features = corrmat.index\n",
    "    plt.figure(figsize=(20,20))\n",
    "    #plot heat map\n",
    "    graph = sns.heatmap( data[top_corr_features].corr(), annot=True, cmap=\"YlGnBu\")\n",
    "\n",
    "heat_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection():\n",
    "    \n",
    "    base_path = \"/Users/heejoojin/Projects/ML/CS4641/\"\n",
    "    file_path = base_path + \"heart.csv\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    num_channels = data.shape[1] - 1\n",
    "    \n",
    "    # print(data)\n",
    "    \n",
    "    channel_list = list(np.arange(0, num_channels))\n",
    "    chosen_features = []\n",
    "    \n",
    "    for i, column in enumerate(data.columns):\n",
    "        \n",
    "        if (column != \"target\"):\n",
    "            print(\"Finding feature #\" + str(i) + ': ')\n",
    "            channel_scores = {}\n",
    "            \n",
    "            for channel in channel_list:\n",
    "                \n",
    "                testing_features = chosen_features + [channel]\n",
    "                feature_list = testing_features\n",
    "                \n",
    "                X_data = data.iloc[:, feature_list]\n",
    "                Y_data = data.iloc[:, -1]\n",
    "                \n",
    "                scaler = MinMaxScaler()\n",
    "                X_scaled_data = scaler.fit_transform(X_data)\n",
    "                X_data = X_scaled_data\n",
    "\n",
    "                X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.2, shuffle=True)\n",
    "\n",
    "                model = Sequential()\n",
    "                model.add(Dense(60, activation='linear', input_shape=(X_train.shape[1],)))\n",
    "                model.add(Dense(1, activation='sigmoid'))\n",
    "                checkpoint_filepath = base_path + \"checkpoint.hdf5\"\n",
    "                early_stopping_callback = EarlyStopping(monitor='val_acc', min_delta=0, patience=5, verbose=0)\n",
    "                checkpoint = ModelCheckpoint(checkpoint_filepath, monitor='val_acc', verbose=0, save_best_only=True,\n",
    "                                        save_weights_only=True, mode='auto', period=1)\n",
    "                callbacks_list = [checkpoint, early_stopping_callback]\n",
    "\n",
    "                # Compile model\n",
    "                model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['mae'])\n",
    "                history = model.fit(X_train, Y_train, epochs=300, batch_size=32, validation_split=0.2, shuffle=True, verbose=0)\n",
    "\n",
    "                # Y_pred = model.predict(np.array(X_test))\n",
    "                scores = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "                \n",
    "            channel_scores[channel] = scores[1]\n",
    "\n",
    "        #find max score\n",
    "        highest_channel = min(channel_scores, key=channel_scores.get)\n",
    "        print(str(highest_channel) + \"\\t\" + str(channel_scores[highest_channel]))\n",
    "        #append channel to list\n",
    "        chosen_features.append(highest_channel)\n",
    "        print(chosen_features)\n",
    "        if i != len(data.columns - 1):\n",
    "            channel_list.remove(highest_channel)\n",
    "            \n",
    "        save_path = base_path + \"feature_selection.csv\"\n",
    "        with open(save_path, 'a') as f:\n",
    "            for item in chosen_features:\n",
    "                f.write(str(item) + \", \")\n",
    "            f.write(\"\\t\" + str(channel_scores[highest_channel]) + \"\\n\\r\")\n",
    "        f.close()\n",
    "# feature_selection()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model():\n",
    "    X_train, X_test, Y_train, Y_test, X_data, Y_data = get_data_post_feature_selection()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, activation='linear', input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    checkpoint_filepath = base_path + \"checkpoint.hdf5\"\n",
    "    \n",
    "    early_stopping_callback = EarlyStopping(monitor='val_acc', min_delta=0, patience=5, verbose=0)\n",
    "    checkpoint = ModelCheckpoint(checkpoint_filepath, monitor='val_acc', verbose=0, save_best_only=True,\n",
    "                            save_weights_only=True, mode='auto', period=1)\n",
    "    # checkpoint = save the model after every epoch.\n",
    "    # reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.001, verbos=0)\n",
    "    \n",
    "    callbacks_list = [checkpoint, early_stopping_callback]\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    history = model.fit(X_train, Y_train, epochs=300, batch_size=32, validation_split=0.2, shuffle=True, verbose=0)\n",
    "    \n",
    "    # Y_pred = model.predict(np.array(X_test))\n",
    "    test_loss, test_acc = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "    print(test_loss)\n",
    "    print(test_acc)\n",
    "\n",
    "    val_acc = history.history['val_acc']\n",
    "    val_loss = history.history['val_loss']\n",
    "    acc = history.history['acc']\n",
    "    loss = history.history['loss']\n",
    "    plt.plot(val_loss, label='validation loss')\n",
    "    plt.plot(val_acc, label='validation accuracy')\n",
    "    plt.plot(acc, label='accuracy')\n",
    "    plt.plot(loss, label='loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # model.load_weights(checkpoint_filepath)\n",
    "    # model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    \n",
    "nn_model()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM():\n",
    "    X_train, X_test, Y_train, Y_test, X_data, Y_data = get_data()\n",
    "    \n",
    "    svm_model_linear = LinearSVC(random_state=0, tol=1e-5)\n",
    "    # tol : float, optional (default=1e-4) = Tolerance for stopping criteria.\n",
    "    model = svm_model_linear.fit(X_train, Y_train.values.flatten())\n",
    "    \n",
    "    Y_pred = svm_model_linear.predict(X_test)\n",
    "    scores = cross_val_score(svm_model_linear, X_train, Y_train, cv=10)\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    cm = confusion_matrix(Y_test, Y_pred)\n",
    "    sns.heatmap(cm, annot=True)\n",
    "    \n",
    "    print(accuracy)\n",
    "    \n",
    "SVM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
